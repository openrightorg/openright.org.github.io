## Mixture of Experts

![](transformer_moe.webp)

In a mixture of experts architecture like Mixtral, multiple feed forward networks constitute experts, each with a unique focus. Gate weights select a fixed number of experts and their relevance.  The input is passed through each of these experts and the outputs are weghted by relevance and added together.
